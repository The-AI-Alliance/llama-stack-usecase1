services:
  ollama:
    image: docker.io/ollama/ollama:latest
    ports:
      - 7869:11434
    volumes:
      - .:/code
      - ./ollama/ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: always
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    networks:
      - ollama-docker

  llama-stack:
    image: llamastack/distribution-ollama:latest
    ports:
      - 5000:5000
    volumes:
      - ./.llama:/root/.llama
      - ./llama-stack-config.yaml:/app/llama-stack-config.yaml:ro
    container_name: llama-stack
    pull_policy: always
    restart: always
    environment:
      - INFERENCE_MODEL=${INFERENCE_MODEL:-llama3.2:1b}
      - OLLAMA_URL=http://ollama:11434
      - LLAMA_STACK_PORT=5000
    command: --port 5000 --config /app/llama-stack-config.yaml
    depends_on:
      - ollama
    networks:
      - ollama-docker

networks:
  ollama-docker:
    driver: bridge