services:
  ollama:
    image: docker.io/ollama/ollama:latest
    platform: linux/arm64
    ports:
      - ${OLLAMA_PORT:-7869}:${OLLAMA_INTERNAL_PORT:-11434}
    volumes:
      - .:/code
      - ./ollama/ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: always
    environment:
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-24h}
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0}
    networks:
      - ollama-docker

  llama-stack:
    image: llamastack/distribution-ollama:latest
    ports:
      - ${LLAMA_STACK_PORT:-5001}:5000
    volumes:
      - ./.llama:/root/.llama
    container_name: llama-stack
    pull_policy: always
    restart: always
    environment:
      - INFERENCE_MODEL=${INFERENCE_MODEL:-llama3.2:1b}
      - OLLAMA_URL=http://ollama:${OLLAMA_INTERNAL_PORT:-11434}
      - LLAMA_STACK_PORT=5000
    command: --port 5000 --template ollama
    depends_on:
      - ollama
    networks:
      - ollama-docker

  llama-stack-playground:
    build:
      context: .
      dockerfile: Dockerfile.playground
    ports:
      - ${PLAYGROUND_PORT:-8501}:${PLAYGROUND_PORT:-8501}
    volumes:
      - ./.llama:/root/.llama
    container_name: llama-stack-playground
    restart: always
    environment:
      - LLAMA_STACK_ENDPOINT=http://llama-stack:5000
    depends_on:
      - llama-stack
    networks:
      - ollama-docker

networks:
  ollama-docker:
    driver: bridge
