services:
  ollama:
    image: docker.io/ollama/ollama:latest
    ports:
      - 7869:11434
    volumes:
      - .:/code
      - ./ollama/ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: always
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    networks:
      - ollama-docker

  llama-stack:
    image: llamastack/distribution-ollama:latest
    ports:
      - 5000:5000
    volumes:
      - ./.llama:/root/.llama
    container_name: llama-stack
    pull_policy: always
    restart: always
    environment:
      - INFERENCE_MODEL=${INFERENCE_MODEL:-llama3.2:3b}
      - OLLAMA_URL=http://ollama:11434
      - LLAMA_STACK_PORT=5000
    command: --port 5000 --template ollama
    depends_on:
      - ollama
    networks:
      - ollama-docker

  llama-stack-playground:
    build:
      context: .
      dockerfile: Dockerfile.playground
    ports:
      - 8501:8501
    volumes:
      - ./.llama:/root/.llama
    container_name: llama-stack-playground
    restart: always
    environment:
      - LLAMA_STACK_ENDPOINT=http://llama-stack:5000
    depends_on:
      - llama-stack
    networks:
      - ollama-docker

networks:
  ollama-docker:
    driver: bridge